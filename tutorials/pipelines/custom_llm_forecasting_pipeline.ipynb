{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Integrating a Custom LLM with SigLLM\n",
        "\n",
        "This tutorial shows how to use your own LLM backend (like Azure OpenAI, Anthropic, or an internal API) with SigLLM's forecasting pipeline.\n",
        "\n",
        "## Overview\n",
        "\n",
        "SigLLM provides a `CustomForecast` primitive that works with any LLM. You just need to:\n",
        "1. Implement a client class with a `generate()` method\n",
        "2. Pass it to the `custom_detector` pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Implement Your LLM Client\n",
        "\n",
        "Create a class that inherits from `BaseLLMClient`. Here's an example for Azure OpenAI:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sigllm.primitives.llm_client import BaseLLMClient\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "class AzureGPTClient(BaseLLMClient):\n",
        "    \"\"\"Azure OpenAI client for SigLLM.\"\"\"\n",
        "    \n",
        "    def __init__(self, endpoint, api_key, deployment, api_version=\"2024-02-15-preview\"):\n",
        "        self.endpoint = endpoint\n",
        "        self.api_key = api_key\n",
        "        self.deployment = deployment\n",
        "        self.api_version = api_version\n",
        "    \n",
        "    def generate(self, prompts, system_message=None, max_tokens=100, \n",
        "                 temperature=1.0, n_samples=1, **kwargs):\n",
        "        \"\"\"Generate responses for a batch of prompts.\"\"\"\n",
        "        all_responses = []\n",
        "        for prompt in prompts:\n",
        "            messages = []\n",
        "            if system_message:\n",
        "                messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "            \n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.deployment,\n",
        "                messages=messages,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                n=n_samples,\n",
        "            )\n",
        "            texts = [choice.message.content for choice in response.choices]\n",
        "            all_responses.append(texts)\n",
        "        \n",
        "        return all_responses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (50, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3600</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7200</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10800</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14400</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   timestamp  value\n",
              "0          0    0.0\n",
              "1       3600   10.0\n",
              "2       7200   20.0\n",
              "3      10800   30.0\n",
              "4      14400   40.0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Simple linear test data\n",
        "np.random.seed(42)\n",
        "n_points = 50\n",
        "data = pd.DataFrame({\n",
        "    \"timestamp\": np.arange(n_points) * 3600,\n",
        "    \"value\": np.linspace(0, n_points-1, n_points) * 10,\n",
        "})\n",
        "\n",
        "print(\"Data shape:\", data.shape)\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Run the Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline primitives:\n",
            "\t0: mlstars.custom.timeseries_preprocessing.time_segments_aggregate\n",
            "\t1: sklearn.impute.SimpleImputer\n",
            "\t2: sigllm.primitives.transformation.Float2Scalar\n",
            "\t3: sigllm.primitives.forecasting.custom.rolling_window_sequences\n",
            "\t4: sigllm.primitives.transformation.format_as_string\n",
            "\t5: sigllm.primitives.forecasting.custom.CustomForecast\n",
            "\t6: sigllm.primitives.transformation.format_as_integer\n",
            "\t7: sigllm.primitives.transformation.Scalar2Float\n",
            "\t8: sigllm.primitives.postprocessing.aggregate_rolling_window\n",
            "\t9: numpy.reshape\n",
            "\t10: orion.primitives.timeseries_errors.regression_errors\n",
            "\t11: orion.primitives.timeseries_anomalies.find_anomalies\n"
          ]
        }
      ],
      "source": [
        "from mlblocks import MLPipeline\n",
        "\n",
        "client = AzureGPTClient(\n",
        "    endpoint=AZURE_ENDPOINT,\n",
        "    api_key=AZURE_API_KEY,\n",
        "    deployment=AZURE_DEPLOYMENT,\n",
        ")\n",
        "\n",
        "pipeline = MLPipeline(\n",
        "    \"custom_detector\",\n",
        "    init_params={\n",
        "        \"sigllm.primitives.forecasting.custom.CustomForecast#1\": {\n",
        "            \"client\": client,\n",
        "            \"steps\": 1,\n",
        "            \"temp\": 0.3,\n",
        "            \"samples\": 4,\n",
        "        },\n",
        "        \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
        "            \"time_column\": \"timestamp\",\n",
        "            \"interval\": 3600,\n",
        "        },\n",
        "        \"sigllm.primitives.forecasting.custom.rolling_window_sequences#1\": {\n",
        "            \"window_size\": 10,\n",
        "            \"target_column\": 0,\n",
        "            \"target_size\": 1,\n",
        "        },\n",
        "        \"sigllm.primitives.transformation.format_as_integer#1\": {\n",
        "            \"trunc\": 1,\n",
        "            \"errors\": \"coerce\"\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Pipeline primitives:\")\n",
        "for i, p in enumerate(pipeline.primitives):\n",
        "    print(f\"\\t{i}: {p}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions shape: (40, 4, 1)\n",
            "Actuals shape: (40, 1)\n",
            "Window 0: pred=100.0, actual=100.0, error=0.0\n",
            "Window 1: pred=110.0, actual=110.0, error=0.0\n",
            "Window 2: pred=120.0, actual=120.0, error=0.0\n",
            "Window 3: pred=130.0, actual=130.0, error=0.0\n",
            "Window 4: pred=140.0, actual=140.0, error=0.0\n"
          ]
        }
      ],
      "source": [
        "context = pipeline.fit(data, output_=7)\n",
        "\n",
        "y_hat = context.get('y_hat')\n",
        "y = context.get('y')\n",
        "\n",
        "print(f\"Predictions shape: {y_hat.shape}\")\n",
        "print(f\"Actuals shape: {y.shape}\")\n",
        "\n",
        "for i in range(min(5, len(y_hat))):\n",
        "    pred = float(np.array(y_hat[i]).flatten()[0])\n",
        "    actual = float(np.array(y[i]).flatten()[0])\n",
        "    print(f\"Window {i}: pred={pred:.1f}, actual={actual:.1f}, error={pred-actual:.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The `generate()` Interface\n",
        "\n",
        "Your `generate()` method must accept these parameters:\n",
        "\n",
        "| Parameter | Type | Description |\n",
        "|-----------|------|-------------|\n",
        "| `prompts` | `List[str]` | List of prompt strings |\n",
        "| `system_message` | `str` or `None` | Optional system context |\n",
        "| `max_tokens` | `int` | Max tokens per response |\n",
        "| `temperature` | `float` | Sampling temperature |\n",
        "| `n_samples` | `int` | Responses per prompt |\n",
        "| `**kwargs` | | Additional parameters |\n",
        "\n",
        "**Returns:** `List[List[str]]` â€” for each prompt, a list of `n_samples` response strings.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "orion310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
